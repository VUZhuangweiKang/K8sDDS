<?xml version="1.0"?>
<dds xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
     xsi:noNamespaceSchemaLocation="http://community.rti.com/schema/6.0.1/6.0.0/rti_routing_service.xsd">

    <routing_service name="rtiperftest_rs">

        <annotation>
            <documentation>
                Routing service for rtiperftest benchmarking application
            </documentation>
        </annotation>

        <administration>
            <domain_id>0</domain_id>
        </administration>

        <domain_route name="RRSConfig" enabled="true">
            <participant name="1">
                <domain_id>1</domain_id>
            </participant>
            <participant name="2">
                <domain_id>2</domain_id>
            </participant>

            <session name="Session" enabled="true">

                <auto_topic_route name="PerftestAutoRoute" enabled="true">
                    <input>
                        <allow_topic_name_filter>*</allow_topic_name_filter>
                        <allow_registered_type_name_filter>*</allow_registered_type_name_filter>
                        <datareader_qos>

                            <!--
                            Setting history.kind = KEEP_ALL, Connext will attempt to maintain
                            and deliver all the values of the instance to existing subscribers.
                            The resources that Connext can use to keep this history are limited
                            by the settings of the RESOURCE_LIMITS.
                            -->
                            <history>
                                <kind>KEEP_ALL_HISTORY_QOS</kind>
                            </history>

                            <!--
                            Setting reliability.kind = RELIABLE_RELIABILITY_QOS, data samples
                            originating from a single DataWriter cannot be made available to the
                            DataReader if there are previous data samples that have not been
                            received yet due to a communication error.

                            The default value of reliability.kind is RELIABLE_RELIABILITY_QOS.
                            The reliability.kind can be modified to BEST_EFFORT_RELIABILITY_QOS
                            by using the command-line option -bestEffort.
                            -->
                            <reliability>
                                <kind>RELIABLE_RELIABILITY_QOS</kind>
                            </reliability>

                            <!--
                            DurabilityQos policy specifies whether or not Connext will store and
                            deliver previously published data samples to new DataReaders
                            that join the network later.

                            Important: durability.kind and durability.direct_communication are configured
                            automatically within the source code. To modify them, use the input commands
                            -durability and -noDirectCommunication.

                            The default value of _DirectCommunication is true.
                            The default value of _Durability is DDS_VOLATILE_DURABILITY_QOS.

                            <durability>
                            <kind>_Durability</kind>
                            <direct_communication>_DirectCommunication</direct_communication>
                            </durability>
                            -->

                            <resource_limits>
                                <!--
                                The initial and maximum number of data samples. The middleware will
                                make sure to allocate space for the initial_samples, and then if
                                needed, it will grow the allocated memory up to a point where it
                                supports max_samples.

                                For the initial number of samples we choose a number that should be
                                enough for most use-cases (therefore no need to grow), but that
                                should not affect the memory consumption by reserving too much
                                memory.
                                -->
                                <max_samples>10000</max_samples>
                                <initial_samples>128</initial_samples>

                                <!--
                                The maximum number of samples that can be stored for a single
                                instance. If the throughput topic is not keyed, there is only a
                                single instance, so this value should always be set the same
                                as max_samples.

                                For a keyed topic, you might want to use this parameter to institute
                                a degree of "fairness" among the instances.
                                -->
                                <max_samples_per_instance>10000</max_samples_per_instance>
                            </resource_limits>

                            <reader_resource_limits>
                                <!--
                                The maximum number of samples that Connext will store from a
                                single DataWriter. If you run this application with only a single
                                DataWriter (that is, in a one-to-one or one-to-many configuration),
                                there is no reason for this value to be set to anything less than
                                max_samples. If you have many writers and need to institute
                                a degree of "fairness" among them, you can decrease this value.
                                -->
                                <max_samples_per_remote_writer>10000</max_samples_per_remote_writer>

                                <!--
                                The maximum number of data samples that the application can receive
                                from Connext in a single call to DataReader::read() or
                                take(). If more data exists in the middleware, the application will
                                need to issue multiple read()/take() calls.

                                When reading data using listeners, the expected number of samples
                                available for delivery in a single take() call is typically small:
                                usually just one in the case of unbatched data, or the number of
                                samples in a single batch in the case of batched data. When polling
                                for data or using Waitsets, however, multiple samples (or batches)
                                could be retrieved at once, depending on the data rate.

                                A larger value for this parameter makes the API simpler to use, at
                                the expense of some additional memory consumption.
                                -->
                                <max_samples_per_read>65536</max_samples_per_read>
                            </reader_resource_limits>

                            <protocol>
                                <rtps_reliable_reader>
                                    <!--
                                    When the DataReader receives a heartbeat from a DataWriter
                                    (indicating (a) that the DataWriter still exists on the network
                                    and (b) what sequence numbers it has published), the following
                                    parameters indicate how long it will wait before replying with
                                    a positive (assuming they aren't disabled) or negative
                                    acknowledgement.

                                    The time the reader waits will be a random duration between
                                    the minimum and maximum values. Narrowing this range, and shifting
                                    it towards zero, will make the system more reactive. However, it
                                    increases the chance of (N)ACK spikes. The higher the fanout in
                                    your system (i.e., the number of readers per writer), the more
                                    you should consider specifying a range here.
                                    -->
                                    <min_heartbeat_response_delay>
                                        <sec>DURATION_ZERO_SEC</sec>
                                        <nanosec>DURATION_ZERO_NSEC</nanosec>
                                    </min_heartbeat_response_delay>
                                    <max_heartbeat_response_delay>
                                        <sec>DURATION_ZERO_SEC</sec>
                                        <nanosec>DURATION_ZERO_NSEC</nanosec>
                                    </max_heartbeat_response_delay>
                                </rtps_reliable_reader>
                            </protocol>
                        </datareader_qos>
                    </input>
                    <output>
                        <allow_topic_name_filter>*</allow_topic_name_filter>
                        <allow_registered_type_name_filter>*</allow_registered_type_name_filter>
                        <datawriter_qos>
                            <writer_resource_limits>
                                <max_remote_reader_filters>256</max_remote_reader_filters>
                            </writer_resource_limits>
                            <!--
                            Setting history.kind = KEEP_ALL, Connext will attempt to maintain
                            and deliver all the values of the instance to existing subscribers. 
                            The resources that Connext can use to keep this history are limited
                            by the settings of the RESOURCE_LIMITS.
                            -->
                            <history>
                                <kind>KEEP_ALL_HISTORY_QOS</kind>
                            </history>
                            <!--
                            Setting reliability.kind = RELIABLE_RELIABILITY_QOS, data samples 
                            originating from a single DataWriter cannot be made available to the
                            DataReader if there are previous data samples that have not been 
                            received yet due to a communication error.

                            The default value of reliability.kind is RELIABLE_RELIABILITY_QOS.
                            The reliability.kind can be modified to BEST_EFFORT_RELIABILITY_QOS
                            by using the command-line option -bestEffort.
                            -->
                            <reliability>
                                <kind>RELIABLE_RELIABILITY_QOS</kind>
                                <max_blocking_time>
                                    <sec>DURATION_INFINITE_SEC</sec>
                                    <nanosec>DURATION_INFINITE_NSEC</nanosec>
                                </max_blocking_time>
                            </reliability>

                            <resource_limits>
                                <!--
                                The number of data samples for which the DataWriter will allocate
                                space. The throughput test is configured without durability, meaning
                                that when all DataReaders have acknowledged a sample, the DataWriter
                                will discard it. The values below, then, effectively indicate how
                                far ahead of the slowest reader the writer is able to get before it
                                will block waiting for the reader(s) to catch up.

                                See the parallel DataReaderQos comment above for more information
                                about the relationships between these three values.
                                
                                Important: These settings are configured automatically within the 
                                source code.
                                    _SendQueueSize = 50 (default)
                                    _SendQueueSize can be configured using the command-line option -sendQueueSize <value>
                                
                                if batching is not set:
                                    <max_samples>_SendQueueSize</max_samples>
                                    <initial_samples>_SendQueueSize</initial_samples>
                                    <max_samples_per_instance>_SendQueueSize</max_samples_per_instance>
                                    
                                if batching is set:
                                    <max_samples>DDS_LENGTH_UNLIMITED</max_samples>
                                    <initial_samples>_SendQueueSize</initial_samples>
                                    <max_samples_per_instance>DDS_LENGTH_UNLIMITED</max_samples_per_instance>
                                -->
                            </resource_limits>
                            
                            <protocol>
                                <rtps_reliable_writer>
                                    <!--
                                    When the writer's cache gets down to this number of samples, it
                                    will slow the rate at which it sends heartbeats to readers.
                                    
                                    Important: This setting is configured automatically within the source code.
                                    <low_watermark>_SendQueueSize * 0.1</low_watermark>
                                    -->

                                    <!--
                                    When the writer's cache is filled to this level, it will begin
                                    sending heartbeats at a faster rate in order to spur faster
                                    acknowledgements (positive or negative) of its samples to allow it
                                    to empty its cache and avoid blocking.
                                    
                                    Important: This setting is configured automatically within the source code.
                                    <high_watermark>_SendQueueSize * 0.9</high_watermark>
                                    -->

                                    <!--
                                    Governs how often heartbeats are "piggybacked" on data samples.
                                    
                                    Important: This setting is configured automatically within the source code.
                                    <heartbeats_per_max_samples>_SendQueueSize * 0.1</heartbeats_per_max_samples>
                                    -->

                                    <!-- 
                                    Minimum and maximum size of send window of unacknowledged samples.

                                    Important: These settings are configured automatically within the source code.
                                    <min_send_window_size>datawriter_qos.resource_limits.max_samples</min_send_window_size>
                                    <max_send_window_size>datawriter_qos.resource_limits.max_samples</max_send_window_size>
                                    -->

                                    <!--
                                    If the number of samples in the writer's cache hasn't risen to
                                    high_watermark, this is the rate at which the DataWriter will
                                    send out periodic heartbeats.
                                    -->
                                    <heartbeat_period>
                                        <sec>DURATION_ZERO_SEC</sec>
                                        <nanosec>10000000</nanosec>
                                    </heartbeat_period>
                                    
                                    <!--
                                    If the number of samples in the writer's cache has risen to
                                    high_watermark, and has not yet fallen to low_watermark, this is
                                    the rate at which the writer will send periodic heartbeats to
                                    its readers.
                                    -->
                                    <fast_heartbeat_period>
                                        <sec>DURATION_ZERO_SEC</sec>
                                        <nanosec>1000000</nanosec>
                                    </fast_heartbeat_period>

                                    <!--
                                    If a durable reader starts up after the writer already has some
                                    samples in its cache, this is the rate at which it will heartbeat
                                    the new reader. It should generally be a shorter period of time
                                    than the normal heartbeat period in order to help the new reader
                                    catch up.
                                    -->
                                    <late_joiner_heartbeat_period>
                                        <sec>DURATION_ZERO_SEC</sec>
                                        <nanosec>10000000</nanosec>
                                    </late_joiner_heartbeat_period>

                                    <!--
                                    The number of times a reliable writer will send a heartbeat to
                                    a reader without receiving a response before it will consider the
                                    reader to be inactive and no longer await acknowledgements before
                                    discarding sent data.
                                    -->
                                    <max_heartbeat_retries>LENGTH_UNLIMITED</max_heartbeat_retries>

                                    <!--
                                    When a DataWriter receives a negative acknowledgement (NACK) from
                                    a DataReader for a particular data sample, it will send a repair
                                    packet to that reader.

                                    The amount of time the writer waits between receiving the NACK and
                                    sending the repair will be a random value between the minimum
                                    and maximum values specified here. Narrowing the range, and
                                    shifting it towards zero, will make the writer more reactive.
                                    However, by leaving some delay you increase the chance that the
                                    writer will learn of additional readers that missed the same data,
                                    in which case it will be able to send a single multicast repair
                                    instead of multiple unicast repairs, thereby using the available
                                    network bandwidth more efficiently. The higher the fanout in your
                                    system (i.e., the more readers per writer), and the greater the
                                    load on your network, the more you should consider specifying a
                                    non-zero delay here.
                                    -->
                                    <min_nack_response_delay>
                                        <sec>DURATION_ZERO_SEC</sec>
                                        <nanosec>DURATION_ZERO_NSEC</nanosec>
                                    </min_nack_response_delay>
                                    <max_nack_response_delay>
                                        <sec>DURATION_ZERO_SEC</sec>
                                        <nanosec>DURATION_ZERO_NSEC</nanosec>
                                    </max_nack_response_delay>
                                </rtps_reliable_writer>
                                
                            </protocol>

                            <!--
                            When sending many small data-samples, you can increated network efficiency 
                            by batching multiple samples together in a single protocol-level message 
                            (usually corresponding to a single network datagram). Batching can offer very 
                            substantial throughput gains, but often at the expense of latency, although 
                            in some configurations, the latency penalty can be very small or zero, 
                            possibly even negative.
                            -->

                            <!-- 
                            Important: This setting is configured automatically within the source code,
                            but only if batching is enabled. _SendQueueSize = 50 (default)
                            <writer_resource_limits>
                            <max_batches>_SendQueueSize</max_batches>
                            </writer_resource_limits>
                            -->

                            <batch>
                                <!--
                                This profile does not enable batching, although the remaining
                                batching settings are configured as if it did. To enable the batch
                                configuration below, turn batching on using the app's command-line
                                or INI file.
                                -->
                                <enable>false</enable>

                                <!--
                                Batches can be "flushed" to the network based on a maximum size.
                                This size can be based on the total number of bytes in the
                                accumulated data samples, the total number of bytes in the
                                accumulated sample meta-data (e.g., timestamps, sequence numbers,
                                etc.), and/or the number of samples. Whenever the first of these
                                limits is reached, the batch will be flushed.
                                
                                Important: This setting is configured automatically within the source code.
                                <max_data_bytes>_BatchSize</max_data_bytes>
                                -->
                                <max_meta_data_bytes>LENGTH_UNLIMITED</max_meta_data_bytes>
                                <max_samples>LENGTH_UNLIMITED</max_samples>

                                <!--
                                The middleware will associate a source timestamp with a batch when
                                it is started. The duration below indicates the amount of time that
                                may pass before the middleware will insert an additional timestamp
                                into the middle of an existing batch.

                                Shortening this duration can give readers increased timestamp
                                resolution. However, lengthening this duration
                                decreases the amount of meta-data on the network, potentially
                                improving throughput, especially if the data samples are very small.
                                If this delay is set to an infinite time period, timestamps will
                                be inserted only once per batch. Furthermore, the middleware will
                                not need to check the time with each sample in the batch, reducing
                                the amount of computation on the send path and potentially improving
                                both latency and throughput performance.
                                -->
                                <source_timestamp_resolution>
                                    <sec>DURATION_INFINITE_SEC</sec>
                                    <nanosec>DURATION_INFINITE_NSEC</nanosec>
                                </source_timestamp_resolution>

                                <!--
                                The maximum flush delay. A batch is flushed automatically after the 
                                delay specified by this parameter. As its value is DURATION_INFINITE, 
                                the flush event will be triggered by max_data_bytes.
                                -->
                                <max_flush_delay>
                                    <sec>DURATION_INFINITE_SEC</sec>
                                    <nanosec>DURATION_INFINITE_NSEC</nanosec>
                                </max_flush_delay>

                                <!--
                                By default, the same DataWriter can be used from multiple threads.
                                If you know that your application will only write data from a single
                                thread, you can switch off a level of locking that occurs when
                                samples are added to a batch. When sending very small samples very
                                fast, this decreased overhead can improve performance.

                                However, even in the case of single-threaded access, the impact of
                                locking can be negligible, and deactivating the lock puts your
                                application at risk of memory corruption if multiple threads do
                                write to the same DataWriter - either without your knowledge or as
                                a result of application maintenance. Therefore, RTI recommends that
                                you only set thread_safe_write to false after detailed testing has
                                confirmed that your application does indeed behave correctly and
                                with improved performance.
                                -->
                                <thread_safe_write>false</thread_safe_write>
                            </batch>

                            <writer_data_lifecycle>
                                <autodispose_unregistered_instances>false</autodispose_unregistered_instances>
                            </writer_data_lifecycle>

                        </datawriter_qos>
                    </output>

                </auto_topic_route>
            </session>
        </domain_route>
    </routing_service>

</dds>
